# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
          import wfdb
import matplotlib.pyplot as plt
from scipy.signal import find_peaks

#to get the directory using id
os.chdir(r'/kaggle/input/chapmanshaoxing-12lead-ecg-database/WFDB_ChapmanShaoxing')

#get the hea file which contain the whole information about the patients
patient = os.listdir(r'/kaggle/input/chapmanshaoxing-12lead-ecg-database/WFDB_ChapmanShaoxing')
patient = list(filter(lambda x : x.split('.')[1] == 'hea',patient ))
df= pd.DataFrame(patient )

#function to get the labels from hea files which is on the 15th line named(dx)
def label_loading(directory):
    with open(directory, 'r') as f:
        lines = f.readlines()
        dx = lines[15].split()
        dx = dx[1].split(',')
        dx = list(map(lambda x : np.int32(x) , dx))
        return dx
  df['diagnosis'] = df[0].apply(label_loading )

#there are alot of patients which have more than one disease so we get the patients who only have just one diseas
sub_df = df[df['diagnosis'].apply(len) == 1]

sub_df.loc[::,'diagnosis'] = sub_df.loc[::,'diagnosis'].apply(lambda x : x[0])

sub_df['diagnosis'].value_counts()
#it is afile named (information) attached with the reseaech paper which contaion the id and medical name of each id in the diagnosis column
label_table = pd.read_csv(r'/kaggle/input/information/ConditionNames_SNOMED-CT.csv')

#we choose the sb and sr as abnormal and normal because they have the maximum number of patients so it will give high accuracy
label_table[label_table['Snomed_CT'] == 426177001  ]
label_table[label_table['Snomed_CT'] == 426783006  ]
our_data = sub_df[(sub_df['diagnosis'] == 426177001) | (sub_df['diagnosis'] == 426783006 )]

our_data
#remove ".hea"
our_data.loc[::,0] = our_data.loc[::,0].apply(lambda x: x.split('.')[0])
our_data
#rename the columns
our_data.columns = ['directory' , 'diagnosis']
#get the ecg record using wfdb library
record = wfdb.rdrecord (our_data.iloc[0,0], sampfrom=0 , sampto=5000)
plt.plot(record.__dict__['p_signal'][::,0])
find_peaks(record.__dict__['p_signal'][::,0], distance=300)
(array([  21,  536, 1056, 1574, 2087, 2571, 3067, 3574, 4079, 4574, 4980]), {})
record.__dict__['p_signal'][0:5000,0]
array([-0.005, -0.02 ,  0.01 , ...,  0.346,  0.342,  0.337])
#record information(12 led ,sampling frequancy 500 ,numper of samples 5000 ,age ,sex ,id, ......)
record.__dict__
#detect qrs(peaks)
from wfdb import processing
processing.qrs.gqrs_detect(record.__dict__['p_signal'][::,0],500)
#variable contain p_signal(array have the values)
ecg=record.__dict__['p_signal'][:,0]
#5000 sample taken from 12 lead
print(record.__dict__['p_signal'].shape)
arr = record.__dict__['p_signal'][::,0]
[179]
#search for any non values
(arr == np.nan).sum()

#number of peaks
len(processing.gqrs_detect(record.__dict__['p_signal'][::,0],500))

our_data
#show the percentage of our data
our_data['diagnosis'].value_counts().plot(kind='pie',autopct="%.1f%%")
plt.title("ECG DISEAS Types Distributions")
plt.legend()
plt.show()
our_data = our_data.replace(426177001, 'SB')
our_data = our_data.replace(426783006, 'SR')

our_data

#remove some samples of our data to avoid overfitting and store the new data in kaggle/working/dataset.csv
count = 0
with open('/kaggle/working/dataset.csv', 'w') as f:
    with open('/kaggle/input/dataset2/imp (1).csv', 'r') as g:
        for line in g:
            if count < 1000 and 'SB' in line:
                count += 1
            else:
                f.write(line)

ecg_data=pd.read_csv(r'/kaggle/working/dataset.csv')

ecg_data
ecg_data['diagnosis'].value_counts().plot(kind='pie',autopct="%.1f%%")
plt.title("ECG DISEAS Types Distributions")
plt.legend()
plt.show()
  #get the p_signal of each patient and store it in (data)
import wfdb
data=[]
for i in range(ecg_data.shape[0]):
    record_name = ecg_data.iloc[i, 1]
    record = wfdb.rdrecord(record_name, sampfrom=0, sampto=5000)
    signals = record.p_signal
    data.append(signals)
    # Do something with the signals here
data[0]
plt.plot(data[0])
len(patient_labels)

len(data)
#convert text to integer (encoding)
ecg_label_1 = []
for i in patient_labels:
    if i=="SB":
        ecg_label_1.append(1)
    elif i=="SR":
        ecg_label_1.append(0)

#convert data and labels to array praparing it for the model
x = np.array(data)
y = np.array(ecg_label_1)
  #splitting the data to train and test
x_train , x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
  x_train.shape[0]

# our cnn model in time domain

from tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, MaxPooling2D, \
GlobalAveragePooling1D, Dense, Dropout, AveragePooling1D
from tensorflow.keras.models import Sequential
from tensorflow.keras.backend import clear_session
from tensorflow.keras.layers import Flatten

model = Sequential()
model.add(Conv2D(128, (3, 3), strides=(3, 3), padding='same', input_shape=(5000, 12, 1),activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(70, (3, 3), strides=(3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(60, (3, 3), padding='same',activation='relu'))
model.add(MaxPooling2D((3, 3), strides=(3, 3), padding='same'))   
model.add(Conv2D(50, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(40, (3, 3), padding='same',activation='relu'))
model.add(Conv2D(50, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(60, (3, 3), padding='same',activation='relu'))
model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(units=600,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=500,activation='relu'))   
model.add(Dense(units=400,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=300,activation='relu'))
model.add(Dense(units=200,activation='relu'))
model.add(Dense(units=100,activation='relu'))
model.add(Dense(units=50,activation='relu'))
model.add(Dense(units=2,activation='softmax'))

model.compile(optimizer="adam",loss="sparse_categorical_crossentropy",
             metrics=["accuracy"])
model.summary()

import os
from keras.callbacks import EarlyStopping, ModelCheckpoint

# Define the directory where you want to save the model
model_dir = os.path.join(os.path.expanduser('~'), 'my_models')

# Create the directory if it doesn't exist
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

# Define the file path for the model checkpoint
model_filepath = os.path.join(model_dir, 'best_model.keras')

# Define the early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3)

# Define the model checkpoint callback
model_checkpoint = ModelCheckpoint(model_filepath, save_best_only=True)

# Train the model with the callbacks
history = model.fit(x_train, y_train, epochs=10, batch_size=16,
                    #callbacks=[early_stopping, model_checkpoint],
                    validation_data=(x_test, y_test))
  ModelLoss, ModelAccuracy = model.evaluate(x_test, y_test)

print('Test Loss is {}'.format(ModelLoss))
print('Test Accuracy is {}'.format(ModelAccuracy ))
25/25 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9923 - loss: 0.0477 
Test Loss is 0.1325867623090744
Test Accuracy is 0.9870298504829407

import pandas as pd

history_df = pd.DataFrame(history.history)

plt.plot( range(history_df.shape[0]), history_df["accuracy"].values, label="Train Accuracy")
plt.plot( range(history_df.shape[0]), history_df["val_accuracy"].values, label="Valid Accuracy")
plt.xlabel("epochs")
plt.ylabel("Accuracy")
plt.grid()
plt.legend()
plt.show()
y_pred = model.predict(x_test)
y_pred_label = [np.argmax(i) for i in y_pred]
y_pred_label[:10]  ## Predicted Labels
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step

[0, 1, 1, 0, 0, 1, 0, 0, 1, 0]

y_test[:10]  ## Actual Labels
array([0, 1, 1, 0, 0, 1, 0, 0, 1, 0])

class_labels = ['SB', 'SR']
plt.figure(figsize=(16,16))
for i in range(16):
    plt.subplot(4,4,i+1)
    plt.plot(x_test[i])
    plt.title(f"Actual Label:{class_labels[y_test[i]]}\nPredicted Label:{class_labels[y_pred_label[i]]}")
    plt.axis("off")


# Compute the confusion matrix
import seaborn as sns
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(y_test, y_pred_label)
# Plot the confusion matrix
plt.figure(figsize=(10,8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()


TP=363
TN=398
FP=1
FN=9
N=FP+TN
P=FN+TP

Sensitivity =TP/(TP + FN)
Sensitivity
0.9758064516129032

Specificity =TN/(TN + FP)
Specificity
0.9974937343358395

FP_rate =FP/N
TP_rate =TP/P


Precision =TP/(TP+FP)

F1 = 2 *((Precision * TP_rate)/(Precision+ TP_rate))
F1
0.9864130434782609

Accuracy =(TP + TN)/(P + N)
Accuracy    
0.9870298313878081

import matplotlib.pyplot as plt

# Sample data
tpr =TP_rate  # True Positive Rate
fpr =FP_rate  # False Positive Rate

# Plotting the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, marker='o', linestyle='-', color='b', label='ROC curve')
plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random classifier')

# Plot annotations
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()


from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_label))
              precision    recall  f1-score   support

           0       1.00      0.98      0.99       407
           1       0.98      1.00      0.99       364

    accuracy                           0.99       771
   macro avg       0.99      0.99      0.99       771
weighted avg       0.99      0.99      0.99       771


cnn model in frequency domain

import wfdb
import numpy as np

dataf = []
for i in range(ecg_data.shape[0]):
    record_name = ecg_data.iloc[i, 1]
    record = wfdb.rdrecord(record_name, sampfrom=0, sampto=5000)
    signals = record.p_signal
    dataf.append(np.fft.fft(signals).real)

x = np.array(dataf)
y = np.array(ecg_label_1)

x_trainf , x_testf, y_trainf, y_testf = train_test_split(x,y,test_size=0.3,random_state=42)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
(1797, 5000, 12)
(771, 5000, 12)
(1797,)
(771,)

model = Sequential()
model.add(Conv2D(128, (3, 3), strides=(3, 3), padding='same', input_shape=(5000, 12, 1),activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(70, (3, 3), strides=(3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(60, (3, 3), padding='same',activation='relu'))
model.add(MaxPooling2D((3, 3), strides=(3, 3), padding='same'))   
model.add(Conv2D(50, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(40, (3, 3), padding='same',activation='relu'))
model.add(Conv2D(50, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(60, (3, 3), padding='same',activation='relu'))
model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(units=600,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=500,activation='relu'))   
model.add(Dense(units=400,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(units=300,activation='relu'))
model.add(Dense(units=200,activation='relu'))
model.add(Dense(units=100,activation='relu'))
model.add(Dense(units=50,activation='relu'))
model.add(Dense(units=2,activation='softmax'))

model.compile(optimizer="adam",loss="sparse_categorical_crossentropy",
             metrics=["accuracy"])
/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(

epochs =10
from keras.callbacks import EarlyStopping, ModelCheckpoint

# Define the early stopping callback
#early_stopping = EarlyStopping(monitor='val_loss', patience=10)

# Define the model checkpoint callback
#model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)

# Train the model with the callbacks
history = model.fit(x_trainf, y_trainf, epochs=epochs, batch_size=16,
                    #callbacks=[early_stopping, model_checkpoint],
                    validation_data=(x_testf, y_testf))
  
ModelLoss, ModelAccuracy = model.evaluate(x_testf, y_testf)

print('Test Loss is {}'.format(ModelLoss))
print('Test Accuracy is {}'.format(ModelAccuracy))
25/25 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.9903 - loss: 0.0240
Test Loss is 0.04770854488015175
Test Accuracy is 0.9805447459220886

[233]
y_pred = model.predict(x_testf)
y_pred_label = [np.argmax(i) for i in y_pred]
y_pred_label[:10]  ## Predicted Labels
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step

[0, 1, 1, 0, 0, 1, 0, 0, 1, 0]

y_testf[:10]  ## Actual Labels
array([0, 1, 1, 0, 0, 1, 0, 0, 1, 0])

import pandas as pd

history_df = pd.DataFrame(history.history)

plt.plot( range(history_df.shape[0]), history_df["accuracy"].values, label="Train Accuracy")
plt.plot( range(history_df.shape[0]), history_df["val_accuracy"].values, label="Valid Accuracy")
plt.xlabel("epochs")
plt.ylabel("Accuracy")
plt.grid()
plt.legend()
plt.show()


# Compute the confusion matrix
import seaborn as sns
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(y_testf, y_pred_label)
# Plot the confusion matrix
plt.figure(figsize=(10,8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()

[238]
TP=353
TN=403
FP=11
FN=4
N=FP+TN
P=FN+TP

Sensitivity2 =TP/(TP + FN)
Sensitivity2
0.988795518207283

Specificity2 =TN/(TN + FP)
Specificity2
0.9734299516908212

FP_rate2 =FP/N
TP_rate2 =TP/P

Precision2=TP/(TP+FP)

F2 = 2 *((Precision * TP_rate)/(Precision+ TP_rate))
F2
0.9864130434782609
[244]
Accuracy =(TP + TN)/(P + N)
Accuracy   
0.980544747081712

import matplotlib.pyplot as plt

# Sample data
tpr1 = TP_rate
fpr1 = FP_rate
tpr2 = TP_rate2
fpr2 = FP_rate2

# Plotting the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr1, tpr1, marker='o', linestyle='-', color='b', label='Point 1')
plt.plot(fpr2, tpr2, marker='o', linestyle='-', color='r', label='Point 2')
plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random classifier')

# Plot annotations
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()




from sklearn.metrics import classification_report
print(classification_report(y_testf, y_pred_label))
              precision    recall  f1-score   support

           0       0.97      0.99      0.98       407
           1       0.99      0.97      0.98       364

    accuracy                           0.98       771
   macro avg       0.98      0.98      0.98       771
weighted avg       0.98      0.98      0.98       771


#our LSTM model in time domain

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Flatten, BatchNormalization, Reshape
from tensorflow.keras.layers import Bidirectional

# Reshape input data to add time dimension
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))

# Define the RNN model
model = Sequential()
model.add(Conv2D(128, (3, 3), strides=(3, 3), padding='same', input_shape=(x_train.shape[1], x_train.shape[2], 1), activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(70, (3, 3), strides=(3, 3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(60, (3, 3), padding='same', activation='relu'))
model.add(MaxPooling2D((3, 3), strides=(3, 3), padding='same'))
model.add(Flatten())  # Flatten the output of the convolutional layers
model.add(Reshape((60, -1)))  # Reshape to match the input shape of the LSTM layer

# Here we should specify the return sequences as True to return the whole sequence rather than the last output
model.add(Bidirectional(LSTM(1024, activation='tanh', return_sequences=True)))
model.add(Bidirectional(LSTM(512, activation='tanh', return_sequences=True)))
model.add(Bidirectional(LSTM(265, activation='tanh', return_sequences=False)))  # Since this is the last LSTM layer, return_sequences should be False

model.add(Dense(265, activation='relu'))
model.add(Dense(40, activation='relu'))
model.add(Dense(30, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(20, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(15, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))  # Change output layer to have 1 neuron and sigmoid activation
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(loss, accuracy))


Test set
  Loss: 0.022
  Accuracy: 0.994


y_pred = model.predict(x_test)
y_pred_label = y_pred
y_pred_label[:10]  ## Predicted Labels
25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step

array([[8.4032974e-05],
       [9.8575979e-01],
       [9.9992359e-01],
       [4.6812474e-07],
       [1.0832588e-06],
       [9.9975878e-01],
       [1.2619397e-07],
       [4.2091528e-07],
       [9.9974853e-01],
       [4.9620780e-07]], dtype=float32)

y_pred = model.predict(x_test)
y_pred_label = np.where(y_pred >= 0.5, 1, 0) # Convert continuous predictions to categorical labels
y_pred_label[:10]
25/25 ━━━━━━━━━━━━━━━━━━━━ 1s 39ms/step

array([[0],
       [1],
       [1],
       [0],
       [0],
       [1],
       [0],
       [0],
       [1],
       [0]])

y_test[:10]  ## Actual Labels
array([0, 1, 1, 0, 0, 1, 0, 0, 1, 0])

# Compute the confusion matrix
import seaborn as sns
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(y_test, y_pred_label)
# Plot the confusion matrix
plt.figure(figsize=(10,8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.show()


TP=360
TN=406
FP=4
FN=1
N=FP+TN
P=FN+TP

Sensitivity3 =TP/(TP + FN)
Sensitivity3
0.997229916897507

Specificity3 =TN/(TN + FP)
Specificity3
0.9902439024390244

FP_rate3 =FP/N
TP_rate3 =TP/P

Precision3=TP/(TP+FP)

F3 = 2 *((Precision * TP_rate)/(Precision+ TP_rate))
F3
0.9864130434782609
[263]
Accuracy3 =(TP + TN)/(P + N)
Accuracy3
0.993514915693904
import matplotlib.pyplot as plt

# Sample data
tpr1 = TP_rate
fpr1 = FP_rate
tpr2 = TP_rate2
fpr2 = FP_rate2
tpr3 = TP_rate3
fpr3 = FP_rate3

# Plotting the ROC curves
plt.figure(figsize=(8, 6))
plt.plot(fpr1, tpr1, marker='o', linestyle='-', color='b', label='Point 1')
plt.plot(fpr2, tpr2, marker='o', linestyle='-', color='r', label='Point 2')
plt.plot(fpr3, tpr3, marker='o', linestyle='-', color='g', label='Point 3')
plt.plot([0, 1], [0, 1], linestyle='--', color='k', label='Random classifier')  # Plot random classifier line for all points

# Plot annotations
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()



from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_label))
              precision    recall  f1-score   support

           0       0.99      1.00      0.99       407
           1       1.00      0.99      0.99       364

    accuracy                           0.99       771
   macro avg       0.99      0.99      0.99       771
weighted avg       0.99      0.99      0.99       771





  
